{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n        \n\n\ndatagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,validation_split=0.2)\n\n\n\ntrain_batches = datagen.flow_from_directory(\n    '/kaggle/input/fire-dataset/fire_dataset/',\n    seed=123,\n    target_size=(224, 224),\n    batch_size=30,\n    subset='training',\n    classes=['fire_images', 'non_fire_images'],\n    class_mode='binary')\n\n\n\n\ntest_batches = datagen.flow_from_directory(\n    '/kaggle/input/fire-dataset/fire_dataset/',\n    seed=123,\n    target_size=(224, 224),\n    batch_size=30,\n    subset='validation',\n    classes=['fire_images', 'non_fire_images'],\n    class_mode='binary')\n\n\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.models import Sequential\n\n\nmodel = Sequential()\nmodel.add(Conv2D(30, (3, 3), input_shape=(224,224,3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(30, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(30, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\nmodel.add(Dense(60))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n\nmodel_trained=model.fit(\n        train_batches,\n        steps_per_epoch=20 // 10,\n        epochs=10,\n        validation_data=test_batches,\n        validation_steps=10 // 10)\n\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-26T20:21:18.841081Z","iopub.execute_input":"2022-07-26T20:21:18.841784Z","iopub.status.idle":"2022-07-26T20:22:22.412448Z","shell.execute_reply.started":"2022-07-26T20:21:18.841685Z","shell.execute_reply":"2022-07-26T20:22:22.411370Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Found 800 images belonging to 2 classes.\nFound 199 images belonging to 2 classes.\nEpoch 1/10\n2/2 [==============================] - 9s 5s/step - loss: 1.9991 - accuracy: 0.7000 - val_loss: 0.7320 - val_accuracy: 0.5667\nEpoch 2/10\n2/2 [==============================] - 5s 4s/step - loss: 0.8297 - accuracy: 0.8000 - val_loss: 0.5863 - val_accuracy: 0.7000\nEpoch 3/10\n2/2 [==============================] - 6s 5s/step - loss: 0.5868 - accuracy: 0.6667 - val_loss: 0.4478 - val_accuracy: 0.8333\nEpoch 4/10\n2/2 [==============================] - 4s 3s/step - loss: 0.3942 - accuracy: 0.8667 - val_loss: 0.4209 - val_accuracy: 0.8000\nEpoch 5/10\n2/2 [==============================] - 5s 3s/step - loss: 0.3477 - accuracy: 0.8667 - val_loss: 0.2240 - val_accuracy: 0.9333\nEpoch 6/10\n2/2 [==============================] - 7s 5s/step - loss: 0.3037 - accuracy: 0.9167 - val_loss: 0.4305 - val_accuracy: 0.7667\nEpoch 7/10\n2/2 [==============================] - 8s 5s/step - loss: 0.4177 - accuracy: 0.8167 - val_loss: 0.4937 - val_accuracy: 0.7000\nEpoch 8/10\n2/2 [==============================] - 5s 3s/step - loss: 0.4214 - accuracy: 0.7167 - val_loss: 0.3300 - val_accuracy: 0.9000\nEpoch 9/10\n2/2 [==============================] - 6s 3s/step - loss: 0.2734 - accuracy: 0.9333 - val_loss: 0.4668 - val_accuracy: 0.8333\nEpoch 10/10\n2/2 [==============================] - 6s 5s/step - loss: 0.1985 - accuracy: 0.9333 - val_loss: 0.3421 - val_accuracy: 0.8333\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}